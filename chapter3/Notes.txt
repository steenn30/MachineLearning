	Other optimization algorithms
		1. newton-cg
		2. lbfgs (limited-memory- Broyden-Fletcher-Goldfarb-Shanno (BFGS)
		3. liblinear
		4. sag
		5. saga
		

	Logistic regression loss is convex, but algs most should converge 
	to the global loss minimum
	
	underfitting (high bias): when our model is not complex enough to capture the pattern 
							  in the training data well and therefore also suffers from 
							  low performance on unseen data
							  
	
	bias-variance tradeoff
	-----------------------------------
	
	- variance: measures the consistentcy (or variability) of the model prediction for 
				classifying a particular example if we retrain the model multiple times
			- We can say that the model is sensitive to the randomness in thet raining data
			
			
	-bias: meausres how far off the predicitons are form the correct values in general if we 
		  rebuild the model multiple times on different training datasets;
		  bias is the measure of systematic erro4r that is not due to randomness
	
	
	More info on these: https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf
	
	
	Regularization is a very useful method for handling collineraity 
	(high correlation among features), filtering out noise from data,
    and eventually preventing overfitting
    
    The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values
    
    L2 regularization(sometimes also called L2 shrinkage or weight decay
    	equation:
				(lambda/2) * ||w||^2 = (lambda/2) [summation(from: j=1, to: m).(w.subscript(j))^2]
				
		lambda is the regularization parameter, which shrinks the weights during model training
		increasing this parameter increases regularization strenght
		- the term c is the inverse
		
		
	
	
	
	Support Vector Machines
	------------------------------------
	- can be considered an extension of the perceptron
	- Perceptron is used to minimize misclassification errors, but SVM maximizes margin
	
	- margin :  distance between the seprating hyperplane (decision boundary) and the training 
				examples that are closest to this hyperplane, which are called support vectors
				
	- large margins typicallyt have a lower general;ization error
	- small margins are more prone to overfitting
	
	Read more about SVMS in:
				*	The NAture of Statistical Learning Theory, Springer Science+Business Media, Vladimir Vapnik, 2000
				
				*	A Tutorial on Support Vector Machines for 
					PAttern Recognition(Data Mining and Knowledge Discovery, 2(2): 121-167, 1998
					
					
	END
	
	Dealing with a nonlineraly sepratable case using slack variables
	---------------------------------------------------------------------------
	Slack variable led to the so-called *soft-margin classification*
	-   The motivation for intorducing the slack variable was that the linear constraints need to be relaxed for nonlinearly 
		separable data to allow the convergence of the optimization in the presence of misclassifications, 
		under appropriate cost penalization
		
	
	END
	
	Train SVM model to classify different flowers:
	----------------------------------
	from sklearn.svm import SVC
	svm = SVC(kernel='linear', C=1.0, random_state=1)
	svm.fit(X_train_std, y_train)
	plot_decision_regions(X_combined_std,
							y_combined,
							classifier=svm
							test_idx=range(105,150)
	plt.xlabel('petal length [standardized]')
	plt.ylabel('petal width [standardized]'
	plt.legend(loc='upper left')
	plt.tight_layout()
	plt.show()
	
	NOTES
	---------------------------
	Linear logistic regression and linear SVMs often yield very similar results. 
	Logistic regression tries to maximize the conditional likelihoods of the training data, 
	which makes it more prone to outliers than SVMs, which mostly care about the points that are closest to 
	the decision boundary(support vectors). 
	
	On the other hand, logistic regression has the advantage that it is a simpler model and can be 
	implemented more easily. Furthermore, logistic regression models can be easily updated, 
	which is attractive when working with streaming data.

	
	LIBLINEAR LIBRARY
	----------------------------
	- Develoed at the National Taiwan University
		(http://www.csie.ntu.edu.tw/~cjlin/liblinear/
	
	LIBSVM
	----------------------------------------
	http://www.csie.ntu.edu.tw/~cjlin/libsvvm
	
	These allow for extremely quick training of large amounts of linear classifiers.
	
	Scikit learn also offers alternative implementations via the SGDClassifier class
		- Supports online learning via the partial_fit method
	
	from sklearn.linear_model import SGDClassifier
	ppn = SGDClassifier(loss='perceptron')
	lr = SGDClassifier(loss='log)
	svm - SGDClassifer(loss='hinge')
	
	SVNs can be *kernalized* to solve nonlinear vclassification
		- this looks like pg 85
	
	The basic idea behind kernel methods to deal with such linearly inseparable data is to create nonlinear 
	combinations of the original features to project them onto a higher-dimensional space via a mapping function, 
	phi(), where the data becomes linearly separable
	
	Uses the following projection:
		phi(x1,x2) = (z1,z2,z3) = (x1,x2,x1^2, x2^2)
	
	
	kernel trick on pg 86-87
	
	Radial basic function (RBf) can simple be called the Gaussian kernel)
	
	The term "kernel" can be interpreted as a similarity function between a pair of examples.
	The minus sign inverts the distance measure into a similarity score, and, due to
	the exponenetial term, the resulting similarity score will fall into a range between
	1(for exactly similar examples) and 0(for very dissimilar examples)
	
	CODE to draw a nonlinear decision boundary that separates the XOR data well
	---------------------------------------------------------------------------
	svm = SVC(kernel='rbf',random_state=1, gamma=0.10, C=10.0)
	svm.fit(x_xor, y_xor)
	plot_decision_regions(X_xor, y_xor, classifier=svm
	plt.legend(loc='upper left')
	plt.tight_layout()
	plt.show()
	
	increasing the value for gamma increases the influence or reach of the training examples
	this leads to a tighter and bumpier decision boundary
		- pics on pg 88 to 89
	
	Information gain and pruning
	
	The three impurity measures or splitting cirteria that are commonly used in binary decision trees are
			1. Gini impurity(Ig) - maximal if the classes are perfectly mixed
			2. entropy(Ih),
			3. clasisifcation error(Ie)
			
		graph code on page 95
		
	
	Combining multiple decision trees via random forest
	------------------------------------------------------------
	Ensemble methods have gained huge popularity in applications of mahcine learning 
	during the last decade due to their good classification performance and robustness toward overfitting
	
	Different ensemble methods include *bagging* and *boosting*
	
	rnadom forest algorithm is known for its good scalability and ease of use
	













