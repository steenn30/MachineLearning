		scikit-learn was created to use NumPy arrays only
			- it can sometimes be more convenient to preprocess data using pandas' DataFrame
			- NumPy is just more mature in scikit-learn
			- you can access NumPy array with DataFrame.values
		
		
		You can drop incomplete rows using DataFrame.dropna(axis=0)
			- axis argument equal to 1 will drop columns with atleast one null/NaN
			
		# only drop rows where all columns are NaN
		# (returns the whole array here since we don't
		# have a row with all values NaN)
		>>> df.dropna(how='all')
		
		#drop rows that have fewwer than 4 real values
		>>> df.dropna(thresh=4)
		
		# only drop rows where NaN appear in specific columns (here: 'C')
		>>> df.dropna(subset=['C'])
		
		Interpolation
			- Might not be a good idea to remove data just because some is missing
			- ***mean interpolation*** is where we simply replace the missing values
									   with the mean values of the entire feature column
								   
		CODE
		----------------------------------------
		>>> from sklearn.impute import SimpleImputer
		>>> import numpy as np
		>>> imr = SimpleImputer(missing_values=np.nan, strategy='mean')
		>>> imr = imr.fit(df.values)
		>>> imputed_data = imr.transform(df.values)
		>>> imputed_data
		
		- Other options for the strategy parameter are media or most-frequent
		- Pandas has a similiar command with "df.fillna(df.mean()"
	
		  SimpleImputer class belongs to the so-called transformer classes in scikit-learn
			- transormer classes are for data transformations.
			- 2 important functions
					- fit : learn the parameters from the training data
					- transform : uses thos parameters to transform the data
			- Any data array that is to be transformed needs to have the same 
			  number of features as the data array that was used to fit the model
			  
			-Estimators are similar to transformers.
				- these have a predict and can also have a transform funciton
			
	
	
		Ordinal vs. Nominal features
		-----------------------------------
		  - ordinal : categorical values that can be sorted or ordered
		  - nominal : these don't usually apply any order
			- T-shirt color is nominal because red is not larger than blue
		
		
		Mapping ordinal features
		----------------------------------
			- To interpret ordinal features correctly, we need to 
			  convert the categorical string values into integers
			  
			- We can simply enumerate class labels since class labels are not ordinal
			- use a reverse-mapping dictionary to convert between the 2 representations
					-LabelEncoder class in scikit-learn can do thsi.
							>>> from sklearn.preprocessing import LabelEncoder
							>>> class_le = LabelEncoder()
							>>> y = class_le.fit_transform(df['classlabel'].values)
							fit_transform is just a shortcut for calling fit and transform separately
							We can use inverse_transofmr to convert back to real class labels
							>>> class_le.inverse_transform(y)
	
		Performing one-hot encoding on nominal features
		--------------------------------------------------
			- Using a LabelEncoder on color, for example, will have the learning algorithm think 
			  that some colors are larger than other colors.
			  
			  The idea behind one-hot encoding is to create a new dummy feature for
			  each unique value in the nominal feature column.
			  
			  so, color blue is represented by [blue=1, red=0, green=0]
			  	- we can use the OneHotEncoder class inside scikit-learn's preprocessing module
		  	
		  	CODE
		  	----------------------------------
		  	>>> from sklearn.preprocessing import OneHotEncoder
		  	>>> X = df[['color,'size', 'price']].values
		  	>>> color_ohe = OneHotEncoder()
		  	>>> color_ohe.fit_transform(X[:,0].reshape(-1,1)).toarray()
		  	
		  	
		  	The above only does work on a single column.
		  	If we want to selectively transform columns in a multi-feature array,
		  	we can use the ColumnTransformer, which accepts a list of (name,transformer,column(s))
		  	
		  	>>> from sklearn.compose import ColumnTransofrmer
		  	>>> X - df[['color', 'size', 'price']].values
		  	>>> c_transf = ColumnTransofrmer([
		  			('onehot', OneHotEncoder(), [0]),
		  			('nothing', 'passthrough', [1,2])
		  	])
		  	>>> c_trans.fit_transform(X).astype(float)
		  	
		  	-   In the above, we specified that we want tomodify only the 
		  		first column and leave the other two columns untouched via 
		  		the 'passthrough' argument	
		  	
		  	- get_dummies method is implemented in pandas
		  		- converts string columns and leaves all other columns unchanged:
		  			>>>pd.get_dummies(df[['price','color','size']])
		  			
		  	
		    - one hot encoding introduces multicolinearity, which is a problem for certain methods
		  		-for example: methods that require matrix inversion
		  		
		    - Highly coorelated matrices are computationally difficult to invert
		  		- this leads to numerically unstable estimates
		  		- reduce coorelation by removing a feature column
		  				-for example: if you remove blue, and the feature column 
		  				 is red=0, green=0, then it must be blue, right?
		  
		    - get_dummies as a param drop_first (true/false)
		    - drop a redundant columne by setting
		  		- drop='first'
		  		- categories='auto'
		  		
		  	Encoding ordinal features
		  	---------------------------------------
		  	Sometimes we are unsure about numerical differences of two categories or the 
		  	difference is undefined, we can also encode using a threshold encoding
		  	with 0/1 values.
		  	
		  	such as "x > M" and x > L"
		  	
		  	We acn apply method of pandas' DataFrames to write custom lambda expressions in order to encode these vairables uisng the value-threshold approach:
		  	
		  	>>> df['x > M'] = df['size'].apply{
		  		...	lambda x: 1 if x in {'L', 'XL'} else 0}
		  
		  	>>> df['x > M'] = df['size'].apply{
		  		...	lambda x: 1 if x in {'L', 'XL'} else 0}		
		  	>>> del df['size']
		  	>>> df
		  
		 
		 
		 	Feature scaling is crucial
		 		- Decision trees and random forests don't need to worry about feature scaling
		 		
		 		2 common approaches:
		 			-normalization : rescaling of features to a range of [0,1] (min/max scaling)
		 				
		 			-standardization :  center the feature columns at mean 0 with standard deviation 1 
		 								so that the feature columns have the same parameters as a 
		 								standard normal distribution (zero mean and unit vairance), 
		 								which makes it easier to learn the weights.
		 								
		 								Furthermore, standardization mainstains useful information 
		 								about outliers and makes the algorithm less sensitive to 
		 								them in constrast to min-max scaling, which scales the data 
		 								to a lmited range of values.
  			StandardScaler class
  			-------------------------
  			>>> from sklearn.preprocessing import StandardScaler
  			>>> stdsc = StandardScaler()
  			>>> X_train_std = stdsc.fit_transform(X_train)
  			>>> X_test_std = stdsc.transform(X_test)
  	
  	
	  		RobustScaler - good for small datasets with many outliers. also good for datasets prone to overfitting.
	  						- RobustScaler removes median value and scales the dataset 
	  						  according to the 1st and 3rd quartile of the dataset 
	  						  (25th and 75th quantile)
	  						- makes more extreme values and outliers less pronounced
	  						
	  						-Resource
	  							- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html
		  		
		  Selecting meaningful Features
		  --------------------------------------
		  	If we notice that a model performs much better on a training dataset than on the test dataset, 
		  	this observation is a strong indicator of overfitting.
		  	
		  	Overfitting means the model fits the parameters too closely with regard to 
		  	the particular observations in the training dataset, nut does not generalize well to new data
		  		- this means the model has a high variance.
		  		
		  	Common solutions to reduce the generalization error are as follows:
		  		- Collect more training data
		  		- Introduce a penalty for complexity via regularization
		  		- Choose a simpler model with fewer parameters
		  		- Reduce the dimensionality of the data
		  	
		  	Here, we will look at common ways to reduce overfitting by regularization and dimensionality 
		  	reduction via feature selection, which leads to simpler models by requiring few parameters	
		  	to be fitted to the data
		  	
		 L1 and L2 regularization as penalties against model complexity
		 -----------------------------------------------------------------
		  - L2 regularization penalizes large individual weights
		  - L1 regularization usually yields sparse feature vectors 
		  	and most feature weights will be zero
		  		
		  - Sparsity can be useful in practice if we have a high-dimensional dataset 
		  	with many features that are irrelevant, espiecally in cases where we have 
		  	more irrelevanbt dimeonsions than training examples	
		  		
		  - Regularization is adding a penalty term to the cost function to encourage smaller weights; 
		  	in other words, we penalize large weights. Thus, bu increasing th regulariazation strength 
		  	via the regularization parameter, lambda, we shrink the weights toward zero and decrease the 
		  	dependence of our model on the training data. Let's illustrate this concept in the following 
		  	figure for the L2 penalty term		
		  	
		  	Great resource for L1 and L2 regularization
		  		: 	Section 3.4, The Elements of Statistical Learning, 
		  			Trevor Hastie, Rober Tibshirani, and Jerome Friedman, 
		  			Springer Science+Business Media, 2009
		  			
		  
		  Sequential feature selection algorithms
		  -----------------------------------------
		  	Alternative way to reduce the complexity of the model and avoid 
			Overfitting is dimensionality reduction via feature selection
				- good for unregualized models
				
			Feature selection - selecting a subset of the oringinal features
			Feature extraction - we derive information from the feature set to construct a new feature spce
			
			Sequential feature selection algorithms are a family of greedy search algorithms
					- These algos are used to reduce an initial d-dimensional 
					feature space to a k-dimensional feature subspace where k<d 
					
				A classic sequential feature selection algorithm is sequential backward selection(SBS), 
				which aims to reduce the dimensionality of the initial feature subspace with a minimum
			 	decay in the performance of the calssifier to improve upon computational efficiency.
			  
			 	In certain casses, SBS can even imporve the predictive poower of the model if a model
			 	suffers from overfitting
			 	
			 	Greedy search algorithms
			 	-----------------------------------
			 	Greddy algorithms make locally optimal choises at each
			 	stage of a combinatorial search problem and generally uyeild a
			 	suboptimal solution to the problem, in constrast to exhaustive search algorithms, which evaluate all possible combinations
			 	and are guaranteed to find the optimal solution. However, in practice, and exhaustive search is often computationally not
			 	feasible, whereas greedy algorithms allow for a less complex,
			 	computationally more efficitent solution
			 	
			 	The idea behind SBS algo:
			 		-sequentially remove features form the full feature subset 
			 		until the new feature subspace constains the desired number of features.
			 		-We use a criterion function, J, that we ant to minimize
			 		
			 	
			 	Outline SBS in four steps:
			 		1. Initialize the algorithm with k=d, where d is the dimensionality of the full feature space, Xd
			 		2. Determine the feature, x^-, that maximizes the criterion: x^- = argmaxJ(Xk -x)
			 			where x is an element of Xk
			 		3. Remove the feature, x^-, from the feature setL X,k-1	- X,k - x^-; k=k-1
			 		4. Terminate if k equals the number of desired features; otherwise, go to step 2.
			 	
			 	A resource on sequential feature algorithms
			 		: Comparative Study of Techniques for Large-Scale Feature Selection, 
			 			F.Ferri, P.Pudil, M.Hatef, and J.Kittler, pages 403-413, 1994
			 			
			 	
			 	SBS algorithm has not been implemented in scikit-learn yet
			 		see file SBS.py in this folder for the code.
			 		
			
			
			
			
			Feature Seelction algorithms in scikit-learn
			--------------------------------------------------------
			book:
				http://scikit-learn.org/stable/modules/feature_selection.html
				
				
			You can find several different flavors of sequential feature 
			selection related to the simple SBNS we implemented previously 
			in the Python package mlxtend
				= http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector