About
-------------------------
	Earlier - learned about the essential machine learning algorithms for classification
			  and how to get our data into shape before we feed it into those algorithm.
			  
	Now 	- Time to learn about the best practices of building good machine learning
			  models by fine-tuning the algorithms and evaluating the
			  performance of the models.
			  
			  
	In this chapter, we will learn how to do the following
			* Assess the performance of machine learning models
			* Diagnose the common problems of machine learning algorithms
			* Fine-tune machine learning models
			* Evaluate predictive models using different performance metrics
			
	
	
	
	Streamlining workflows with pipelines
	-----------------------------------------------------------------
	
		- When we applied different preprocessing techniques, we have to reuse
			the parameters that were obtained during the fitting of the training 
			data to scale and compress any new data, such as the exmaples in the
			separate test dataset.
			
		- A handy tool is the Pipeline class in scikit-learn
				- Pipeline class allows us to fit a model including an arbitrary
					number of transformation steps and apply it to make predictions
					about new data.
	
	Loading the Breast Cancer Wisconsin dataset
	-------------------------------------------------------------------
		- Dataset can be found at:
			- https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data
		- Info on the dataset can be found at:
			- https://archive.ics.uci.edu/m1/datasets/Breast+Cancer+Wisconsin+(Diagnostic).
	
	Combining transformers and estimators in a pipeline
	-------------------------------------------------------------------
		- 	According to the previous chapter: many learning algorithms require
			input features on the same scale for optimal performance.
		- 	We need to standardize the features ( since they are measured on various scales)
			before we feed them to a linear classifier, such as logistic regression.
			
				CODE
				----------------------
				 ABOUT: chaining StandardScaler, PCA, and LogisticRegression objects in a pipeline
				---------------------------------------------------------------------------------------
					from sklearn.preprocessing import Standard Scaler								   
					from sklearn.decomposition import PCA
					from sklearn.linear_model import LogisticRegression
					from sklearn.pipeline import make_pipeline
					pipe_lr = make_pipeline(StandardScaler(),
												PCA(n_components=2),
												LogisticRegression(random_state = 1,
																	solver = 'lbfgs')
					
					pipe_lr.fit(X_train, y_train)
					y_pred = pipe_lr.predict(X_test)
					print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))
				--------------------------------------------------------------------------------
			
			- When we executed the fit method on the pipe_lr pipeline in the preceding code example, 
			  the StandardScaler first performed fit and transform calls on teh training data.
			- Second, the transformed training data was passed on to the next object in the Pipeline,
			  PCA. PCA also executes fit and transform on the scaled input data and passed it to the
			  final element, the estimator
			  	- This is the LogisticRegression estimator
			  	
			 - There are no limits to the number of intermediate steps, but the final element 
			   must be an estimator
			   
			 - Pipelines also implement a predict method.
			 - If we feed a dataset to the predict call of a Pipeline object instance, the data
			   will pass through the intermediate steps via transform calls.
			   		- In the final step, the estimator object will then return a prediction on the 
			   		  transformed data.
			   		  
			 - Refer to the figure on pg 195
	Using k-fold cross-validation to assess model performance
	-------------------------------------------------------------------
	
	The holdout method
	-------------------------------------------------------------------
	
	K-fold cross-validation
	-------------------------------------------------------------------
	
	Debugging algorithms with learning and validation curves
	-------------------------------------------------------------------
	
	Diagnosing bias and variance problems
	-------------------------------------------------------------------
	
	Addressing over- and underfitting with validation curves
	-------------------------------------------------------------------
	
	Fine-tuning machine learning models via grid search
	-------------------------------------------------------------------
	
	
	Tuning hyperparameters via grid search
	-------------------------------------------------------------------
	
	
	Algorithm selection with nested cross-validation
	-------------------------------------------------------------------
	
	Looking at different performance evaluation metrics
	-------------------------------------------------------------------
	
	Reading a confusion matrix
	-------------------------------------------------------------------
	
	Optimizing the precision and recall of a classification model
	-------------------------------------------------------------------
	
	Plotting a receiver operating characteristic
	-------------------------------------------------------------------
	
	Scoring metrics for multiclass classification
	-------------------------------------------------------------------
	
	Dealing with class imbalance
	-------------------------------------------------------------------
	
	Summary
	-------------------------------------------------------------------
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	