About
-------------------------
	Earlier - learned about the essential machine learning algorithms for classification
			  and how to get our data into shape before we feed it into those algorithm.
			  
	Now 	- Time to learn about the best practices of building good machine learning
			  models by fine-tuning the algorithms and evaluating the
			  performance of the models.
			  
			  
	In this chapter, we will learn how to do the following
			* Assess the performance of machine learning models
			* Diagnose the common problems of machine learning algorithms
			* Fine-tune machine learning models
			* Evaluate predictive models using different performance metrics
			
	
	
	
	Streamlining workflows with pipelines
	-----------------------------------------------------------------
	
		- When we applied different preprocessing techniques, we have to reuse
			the parameters that were obtained during the fitting of the training 
			data to scale and compress any new data, such as the exmaples in the
			separate test dataset.
			
		- A handy tool is the Pipeline class in scikit-learn
				- Pipeline class allows us to fit a model including an arbitrary
					number of transformation steps and apply it to make predictions
					about new data.
	
	Loading the Breast Cancer Wisconsin dataset
	-------------------------------------------------------------------
		- Dataset can be found at:
			- https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data
		- Info on the dataset can be found at:
			- https://archive.ics.uci.edu/m1/datasets/Breast+Cancer+Wisconsin+(Diagnostic).
	
	Combining transformers and estimators in a pipeline
	-------------------------------------------------------------------
		- 	According to the previous chapter: many learning algorithms require
			input features on the same scale for optimal performance.
		- 	We need to standardize the features ( since they are measured on various scales)
			before we feed them to a linear classifier, such as logistic regression.
			
				CODE
				----------------------
				 ABOUT: chaining StandardScaler, PCA, and LogisticRegression objects in a pipeline
				---------------------------------------------------------------------------------------
					from sklearn.preprocessing import Standard Scaler								   
					from sklearn.decomposition import PCA
					from sklearn.linear_model import LogisticRegression
					from sklearn.pipeline import make_pipeline
					pipe_lr = make_pipeline(StandardScaler(),
												PCA(n_components=2),
												LogisticRegression(random_state = 1,
																	solver = 'lbfgs')
					
					pipe_lr.fit(X_train, y_train)
					y_pred = pipe_lr.predict(X_test)
					print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))
				--------------------------------------------------------------------------------
			
			- When we executed the fit method on the pipe_lr pipeline in the preceding code example, 
			  the StandardScaler first performed fit and transform calls on teh training data.
			- Second, the transformed training data was passed on to the next object in the Pipeline,
			  PCA. PCA also executes fit and transform on the scaled input data and passed it to the
			  final element, the estimator
			  	- This is the LogisticRegression estimator
			  	
			 - There are no limits to the number of intermediate steps, but the final element 
			   must be an estimator
			   
			 - Pipelines also implement a predict method.
			 - If we feed a dataset to the predict call of a Pipeline object instance, the data
			   will pass through the intermediate steps via transform calls.
			   		- In the final step, the estimator object will then return a prediction on the 
			   		  transformed data.
			   		  
			 - Refer to the figure on pg 195
	Using k-fold cross-validation to assess model performance
	-------------------------------------------------------------------
		- One of the key steps in building a machine learning model is to estimate
		  its performance on data that the model hasn't seen before.
		- A model can suffer from:
				- underfitting (high bias) 	  : if the model is too simple
				- overfitting (high variance) : if the model is too complex for the underlying
											  	training data
				We have to techniques to tackle these problems:
						1. holdout cross-validation
						2. k-fold cross-validation
				These can help us to obtain reliable estimates of the model's generalization performance
						- or rather, how well the model performs on unseen data
	The holdout method
	-------------------------------------------------------------------
			- Using the holdout method
				- We split our initial dataset into seperate training and test datasets
						- former used for model training
						- latter is used to estimate its generalization peformance
				- model selection - tuning and comparing different parameter settings to further improve
									the performance for making predictions on unseen data
								 - the name refers to a given classification problem for which we want to select
								   the optimal values of tuning parameters (also called hyperpraremeters)
				- If we reuse test data enough it will become trianing data.
				- Using the test dataset for model selection is not a good machine learning practice
				
			- A better way of using the holdout method for model selection is to 
			  separate the data into three parts:
			  		1. training dataset 
			  		2. validation dataset
			  		3. test dataset
				- Training dataset is used to fit the different models, and the performance on the 
				  validation dataset is then used for the model selection
				- Using a test dataset the model hasn't seen before during training and model 
				  selection is that we can obtian a less biased estimate of its ability to generalize
				  to new data
				- pg 196 diagram
						- shows the concept of holdout cross-validation, where we use a validation dataset to
						  repeatedly evaluate the performance of the model after training using different
						  hyperparaemeter values
						- Once we are staisfied with the utuning of hyperparameter values,
						  we estimate the model's generalization performance on the test
						  dataset
		  - Disadvantages
		  		- may be very sensistive to how we partitiion the trianing dataset into the training
		  		  and validation subsets; the estimate will vary for different examples of the data
		  		  
		  		  
		  		  
	K-fold cross-validation
	-------------------------------------------------------------------
		- In k-gold cross-validation, we randomly split the training dataset into k folds without
		  replacement.
		  		- k-1 folds are used for the model training
		  		- one fold is used for performance evaluation
	  	- This procedure is repeated k times so that we obtain k models
	  	   and performance estimates
	  	   
		- We then cal.culate the average performance of the models based on the different, independent test folds
		 		- This allows us to obtain a performance estimate that is less sensitive
		 		  to the sub-partitioning of the training data compared to the holdout 
		 		  method
		 - We use k-fold cross-validation for model tuning
		 		- Model tuning is finding the optimal hyperparameter values that yield a satisfying
		 		  generalization performance, which is estimated from evaluating the model performance 
		 		  on the test folds.
		 - We can retrain the model on the complete training dataset to obtain a final performance estimate
		   by using the independent test dataset once we have found satisfactory hyperparameter values.
		   		- providing more training examples to a learning algorithm usually results in a more
		   		  accurate and robust model.
		  - k-fold is a resampling technique without replacement
		  		- By only using each example once in training and validation, we get a lower-variance estimate
		  		  of the model performance than the holdout method
		  - A good standard value for k in k-fold cross-validation is 10.
		  
		  RESOURCE:
		  ----------------------------------------------------------------------------------------------------------
		  		- 	A study of Cross-validation and BNootstrap for accuracy estimation and model Selection, 
		  		  	Kohavi Ron, International Joint Conference on Artifical Intelligence (IJCAI),
		  		  	14 (12): 1137-43, 1995
		  ------------------------------------------------------------------------------------------------------------
		  - If we are working with relatively small trianing sets, it can be useful to
		    increase the number of folds.
		    	- If we increase k, more trianing data will be used in each iteration
		    			- This results in a lower pessimistic bias toward estimating the 
		    			  generalization performance by averaging the individual model estimates
		    			- In contrast, large values of k will also increase the runtime of the cross-validation
		    			  algorithm and yield estimates with higher variance
		    			  		- This is because the training folds will be more similar
		    			  		  to each other
			  		  	- Larger datasets will allow for a smaller value for k, which reduces computational costs
			  		  	
		Leave-one-out-cross-validation
		--------------------------------------------------------------------------------------------------
			- A special case of k-fold cross-validation is the leave-one-out cross-validation (LOOCV) method.
			- In LOOCV, we set the number of folds equal to the the number of training examples
					- (k=n)
			- This makes it so that only one training example is used for testing during each iteration, 
			  which is recommended approach for working with very small datasets
		--------------------------------------------------------------------------------------------------
	
				  
			- A slight improvement over the standard k-fold cross-validation approach is stratified
			  k-fold cross-validation
			  		- This can yield better bias and variance estimates
			  				- especially in cases of unqual class proportions
			 - In stratified cross-validation, the calss label proportions are preserved in each fold to 
			   ensure that each fold is representative of the class proprotions in the training dataset
			   
			 CODE
			 ------------------------------------------------------------
			 import numpy as np
			 from sklearn.model_selection import STratifiedKFold
			 
			 kfold = StratifiedKFold(n_splits=10).split (X_train, y_train)
			 scores = []
			 for k, (train,test) in enumerate(kfold):
			 	pipe_lr.fit(X_train[train], y_train[train])
			 	score = pipe_lr.score(X_train[test], y_train[test])
			 	scores.append(score)
			 	print('Fold: %2d, Class dist.: %s, Acc: %.3f' % (k + 1,
			 		np.bincount(y_train[train]), score))
			 	print('\nCV accuracy: %.3f +/- %.3f' &
			 		(np.mean(scores), np.std(scores)))
			 ------------------------------------------------------------------------
			 	- First we intialized the StratifiedKFold iterator form the 
			 	  sklearn.model_selection module with the y_train class labels in the training dataset,\
			 	  and we specified the number of folds via the n_splits parameter
			 	- When we used the kfold iterator to loop through the kfolds, we used the returned indicies
			 	  in train to fit the logistic regression pipeline that we set up at the start of this
			 	  chapter
			 	- Using the pipe_lr pipeline, we ensured that the examples were scaled properly (for instance,
			 	  standardize) in each iteration. 
			 	- We then used the test indices to calculate the accuracy score of the model
			 			- Which we collected in the scores list to calculate the average accuracy
			 			  and standard deviation of the estimate
			 			  
			 CODE
			 -----------------------------------------------------------------
			 '''
			 	scikit-learn also implements a k-fold cross-validation scorer
			 		- this allows us to evaluate our model using
			 		  stratified k-fold cross-validation less
			 		  verbosely
			 	-An extremely useful feature of the cross_val_score appraoch is that we can distribute the 
			 	 evaluation of the different folds across multiple CPUs.
			 	- If we set the n_jobs paraemeter to 1, only one CPU will be used to evaluate performances
			 			- setting n_jobs=2, we could distribute the 10 rounds of cross-validation
			 			  to two CPUs
			 			- setting n_jobs = -1 allows us to use all available CPUs
			 	
			 '''
			 from sklearn.model_selection import cross_val_score
			 scores = cross_val_score(estimator=pipe_lr,
			 						  X = X_train,
			 						  y=y_train,
			 						  cv=10,
			 						  n_jobs = 1)
			 print('CV accuracy scores: %s' % scores
			 print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores),
			 		np.std(scores)))
			 
			 Estimating generalization performance
			 -------------------------------------------------------------
			 	Resource about model evaluation and cross-validation
			 		- Model evaluation, model selection, and algorithm selecting in machine learning.
			 			Raschka S. arXiv preprint arXiv: 1811.12808, 2018
			 				- https://arxiv.org/abs/1811.12808
			 		- Analysis of Variance of Cross-validation Estimators of the Generalization Error, M. Markatou,
			 			H. Tian, S.Biswas, and G.M. Hripcsak, Journal of Machine Learning Research,
			 				6: 1127-1168, 2005
			 		
			 
	
	Debugging algorithms with learning and validation curves
	-------------------------------------------------------------------
		- Taking a look at two diagnostic tools to improve performance:
				1. Learning Curves - diagnose whether a learning algorithm has a problem with 
									 overfitting (high variance) or underfitting (high bias)
				2. Validation curves - Helps us address common problems
	
	Diagnosing bias and variance problems
	-------------------------------------------------------------------
		- The model tends to overfit the training data and does not generalize well to unseen data 
		  if a model is too complex for a given dataset 
		  	-I.E there are too many degress of freedom or parameters in this model
		  	- It can help to collect more training examples to reduce the degree of overfitting
		
		- By plotting the modcel training and validation accuracies as funciton of the training 
		  dataset size, we can easily detect whether the model sufferes from high variance
		  or high bias, and whether the collection of more data could help to address this
		  problem.
		  
		- pg 202 graph for two common model issues
				- upper-left shows a model with high bias. This modcel has both low trianing
				  and cross-validation accuracy, which indicates that it underfits the training
				  data
						- Common ways to address this issue are to increase the number of parameters 
						  of the model, for example, by collecting or constructing additional features,
						  or by decreasing the degree of regularization
						  	- for example: support vector machine (SVM) or logistic
						  					regression classifiers
				- Thbe graph in the upper-right suffers form high variance
					- This is indicated by the large gap between the trianing and cross-validation accuracy.
					- To address this problem of overfitting, we can collect more training data,
					  reduce the complexity of the model, or increase the regularization parameter
		- Decrease overfitting in unregularized models by feature extraction and feature selection.
		- Collecting more training data usually tends to decrease the chance of overfitting
				- Won't help if data is extremely noisy or the model is already very
				  close to optimal
		- To evaluate the model using a learning curve function,
			- See LearningCurve.py in Chapter 6 folder.
	
	Addressing over- and underfitting with validation curves
	-------------------------------------------------------------------
		- Validation curves are a useful tool for improving the performance of a model
		  by addressing issues such as overfitting or underfitting
		  
		- Instead of plotting the training and test accuracies as functions of the sample size,
		  we vary the values of the model parameters, for example, the inverse regularization parameter,
		  C, in logistic regression
	
	Fine-tuning machine learning models via grid search
	-------------------------------------------------------------------
	
		- 2 types of parameters:
			1. Those that are learned from the training data
					- the weights in logistic regression
			2. Parameters of a learning algorithm that are optimized separately
			
		- The latter are the tuning parameters (or hyperparameters) of a model, for example, the
		  regularization parameter in logistic regression or the depth parameter of a decision tree
		  
		- In this section, we will take a look at a popular hyperparameter optimization technique 
		  called grid search
		 	- Grdi search can further help to improve the performance of a model
		      by finidng  the optimal combination of hyperparameter values
	
	
	Tuning hyperparameters via grid search
	-------------------------------------------------------------------
		- Grid search approach is quite simple
			- It's a brute-force exhaustive search paradigm where we specify a list of values for 
			  different hyperparameters, and the computer evaluates the model performance for each 
			  combination to obtain the optimal combination of values from this set:
			  
			  
				CODE
				--------------------------------------------
				from sklearn.model_selection import GridSearchCV
				from sklearn.svm import SVC
				
				pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1))
				param_range = [0.001, 0.001, 0.01, 0.1, 
							   1.0, 10.0, 100.0, 1000.0]
				
				param_grid = [{'svc__C': param_range,
								'svc__kernel':['linear']},
							{'svc__C' : param_range,
							 'svc__gamma': param_range,
							 'svc__kernel': ['rbf']}]
							 
				gs = GridSearchCV(estimator = pipe_svc,
								  param_grid = param_grid,
								  scoring='accuracy',
								  cv=10,
								  refit= True,
								  n_jobs =-1)
				gs = gs.fit(X_train, y_train)
				print(gs.best_score_)
				print(gs.best_params_)	
		
		- We intialized a GridSearchCV object from the sklearn.model_selection module to train and
		  tune an SVM pipeline
		- We set the param_grid parameter of GridSearchCV to a list of dictionaries to specify the
		  parameters that we'd want to tune.
		- For the linear SVM, we only evaluated the inverse regularization parameter, C; for the RBF kernel SVM
		- We tuned both the svc__C and svc_gamma parameters
			-svc__gamma parameter is specific to kernel SVMs
			
			
		- After using the trianing data to perform the grid search, we obtianed the score
		  of the best-performing model via the best_score_ attribute
		- We can access the best_score_ attribute's value through the best_params_ attribute
		-In this case, the RBF kernel SVM model with svc__C = 100.0 yileded thje best k-fold 
		 cross-validation accuracy: 98.5
		 
		- Finally, we use the independent test dataset to estimate the performance of the best-selected model, 
		  which is available via the best_estimator_ attribute of the GridSearchCV object:
		  
		  		clf = gs.best_estimator_
		  		clf.fit(X_train, y_train)
		  		print('Test accuracy: %.3f' % clf.score(X_test, y_test))
		  		
		 - fitting the model with the best settings (gs.best_estimator) on the trianing set
		   manually via clf.fit(X_train, y_train) after compelteing the grid search is not necessary.
		 - GridSearchCV has a refit parameter
		 	- refit will refit the gs.besT_estimator_ to the whole training set automatically if we set
		 	  refit=True (default
		 	  
		
		NOTE: Randomized hyperparameter search
		------------------------------------------------------
			- An alternative approach for sampling different parameter cobinations using 
			  scikit-learn is randomized search.
			  	- Randomized serach usually performs about as well as grid search, but is much more 
			  	  cost-and time- effective.
						- For exmaple, if we sample 60 parameter combinations via randomized search, 
						  we already have a 95 percenmt probability of obtaining solutions within 5 percent
						  of the optimal performance
						  	-SOURCE:
						  		- Rnadom serach for hyper-parameter optimization, Bergstra K, Bengio
						  			Y. Journal of MAchine Learning Research pp. 281 - 305, 2012
				- Using RandommizedSearchCV, we can draw random parameter combinations from sampling
				  distributions with a specified budget.
				  		- http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization
				  		
				  		
				  		
				  		
	Algorithm selection with nested cross-validation
	-------------------------------------------------------------------
	
		- Combine grid search with k-fold cross-validation for fine-tuning the performance 
		  of a  machine learning model\
		- Nested cross-validation is good to select among different machine learning algorithms.
		
		- The true error of the estimate is almost unbiased releative to the test dataset when 
			nested cross-validation is used
			- SOURCE
			-------------------------------------------------------------------
				- Bias in Error Estimation When Using Cross-Validation for Model Selection, 
					BMC Bioinformatics, S.Varma and R.Simon, 7(1): 91, 2006
		  
	
	Looking at different performance evaluation metrics
	-------------------------------------------------------------------
	
	Reading a confusion matrix
	-------------------------------------------------------------------
	
	Optimizing the precision and recall of a classification model
	-------------------------------------------------------------------
	
	Plotting a receiver operating characteristic
	-------------------------------------------------------------------
	
	Scoring metrics for multiclass classification
	-------------------------------------------------------------------
	
	Dealing with class imbalance
	-------------------------------------------------------------------
	
	Summary
	-------------------------------------------------------------------
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	