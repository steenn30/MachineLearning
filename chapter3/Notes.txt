	Other optimization algorithms
		1. newton-cg
		2. lbfgs (limited-memory- Broyden-Fletcher-Goldfarb-Shanno (BFGS)
		3. liblinear
		4. sag
		5. saga
		

	Logistic regression loss is convex, but algs most should converge 
	to the global loss minimum
	
	underfitting (high bias): when our model is not complex enough to capture the pattern 
							  in the training data well and therefore also suffers from 
							  low performance on unseen data
							  
	
	bias-variance tradeoff
	-----------------------------------
	
	- variance: measures the consistentcy (or variability) of the model prediction for 
				classifying a particular example if we retrain the model multiple times
			- We can say that the model is sensitive to the randomness in thet raining data
			
			
	-bias: meausres how far off the predicitons are form the correct values in general if we 
		  rebuild the model multiple times on different training datasets;
		  bias is the measure of systematic erro4r that is not due to randomness
	
	
	More info on these: https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf
	
	
	Regularization is a very useful method for handling collineraity 
	(high correlation among features), filtering out noise from data,
    and eventually preventing overfitting
    
    The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values
    
    L2 regularization(sometimes also called L2 shrinkage or weight decay
    	equation:
				(lambda/2) * ||w||^2 = (lambda/2) [summation(from: j=1, to: m).(w.subscript(j))^2]
				
		lambda is the regularization parameter, which shrinks the weights during model training
		increasing this parameter increases regularization strenght
		- the term c is the inverse
		
		
	
	
	
	Support Vector Machines
	------------------------------------
	- can be considered an extension of the perceptron
	- Perceptron is used to minimize misclassification errors, but SVM maximizes margin
	
	- margin :  distance between the seprating hyperplane (decision boundary) and the training 
				examples that are closest to this hyperplane, which are called support vectors
				
	- large margins typicallyt have a lower general;ization error
	- small margins are more prone to overfitting
	
	Read more about SVMS in:
				*	The NAture of Statistical Learning Theory, Springer Science+Business Media, Vladimir Vapnik, 2000
				
				*	A Tutorial on Support Vector Machines for 
					PAttern Recognition(Data Mining and Knowledge Discovery, 2(2): 121-167, 1998
					
					
	END
	
	Dealing with a nonlineraly sepratable case using slack variables
	---------------------------------------------------------------------------
	Slack variable led to the so-called *soft-margin classification*
	-   The motivation for intorducing the slack variable was that the linear constraints need to be relaxed for nonlinearly 
		separable data to allow the convergence of the optimization in the presence of misclassifications, 
		under appropriate cost penalization
		
	
	END
	
	Train SVM model to classify different flowers:
	----------------------------------
	from sklearn.svm import SVC
	svm = SVC(kernel='linear', C=1.0, random_state=1)
	svm.fit(X_train_std, y_train)
	plot_decision_regions(X_combined_std,
							y_combined,
							classifier=svm
							test_idx=range(105,150)
	plt.xlabel('petal length [standardized]')
	plt.ylabel('petal width [standardized]'
	plt.legend(loc='upper left')
	plt.tight_layout()
	plt.show()
	
	NOTES
	---------------------------
	Linear logistic regression and linear SVMs often yield very similar results. 
	Logistic regression tries to maximize the conditional likelihoods of the training data, 
	which makes it more prone to outliers than SVMs, which mostly care about the points that are closest to 
	the decision boundary(support vectors). 
	
	On the other hand, logistic regression has the advantage that it is a simpler model and can be 
	implemented more easily. Furthermore, logistic regression models can be easily updated, 
	which is attractive when working with streaming data.

	
	LIBLINEAR LIBRARY
	----------------------------
	- Develoed at the National Taiwan University
		(http://www.csie.ntu.edu.tw/~cjlin/liblinear/
	
	LIBSVM
	----------------------------------------
	http://www.csie.ntu.edu.tw/~cjlin/libsvvm
	
	These allow for extremely quick training of large amounts of linear classifiers.
	
	Scikit learn also offers alternative implementations via the SGDClassifier class
		- Supports online learning via the partial_fit method
	
	from sklearn.linear_model import SGDClassifier
	ppn = SGDClassifier(loss='perceptron')
	lr = SGDClassifier(loss='log)
	svm - SGDClassifer(loss='hinge')
	
	SVNs can be *kernalized* to solve nonlinear vclassification
		- this looks like pg 85
	
	The basic idea behind kernel methods to deal with such linearly inseparable data is to create nonlinear 
	combinations of the original features to project them onto a higher-dimensional space via a mapping function, 
	phi(), where the data becomes linearly separable
	
	Uses the following projection:
		phi(x1,x2) = (z1,z2,z3) = (x1,x2,x1^2, x2^2)
	
	
	kernel trick on pg 86-87
	
	Radial basic function (RBf) can simple be called the Gaussian kernel)
	
	The term "kernel" can be interpreted as a similarity function between a pair of examples.
	The minus sign inverts the distance measure into a similarity score, and, due to
	the exponenetial term, the resulting similarity score will fall into a range between
	1(for exactly similar examples) and 0(for very dissimilar examples)
	
	CODE to draw a nonlinear decision boundary that separates the XOR data well
	---------------------------------------------------------------------------
	svm = SVC(kernel='rbf',random_state=1, gamma=0.10, C=10.0)
	svm.fit(x_xor, y_xor)
	plot_decision_regions(X_xor, y_xor, classifier=svm
	plt.legend(loc='upper left')
	plt.tight_layout()
	plt.show()
	
	increasing the value for gamma increases the influence or reach of the training examples
	this leads to a tighter and bumpier decision boundary
		- pics on pg 88 to 89
	
	Information gain and pruning
	
	The three impurity measures or splitting cirteria that are commonly used in binary decision trees are
			1. Gini impurity(Ig) - maximal if the classes are perfectly mixed
			2. entropy(Ih),
			3. clasisifcation error(Ie)
			
		graph code on page 95
		
	
	Combining multiple decision trees via random forest
	------------------------------------------------------------
	Ensemble methods have gained huge popularity in applications of mahcine learning 
	during the last decade due to their good classification performance and robustness toward overfitting
	
	Different ensemble methods include *bagging* and *boosting*
	
	random forest algorithm: known for its good scalability and ease of use
		- A random forest can be considered as an ensemble of decision trees
	
	Can be summarized in 4 steps:
		1. Draw a random bootstrap sample of size n (rnadomly choose n examples from the trianing dataset with replacement. 
		2. Grow a decisiuon tree from the bootstrap sample. At each node:
			a. Randomly sleect d features without replacement
			b.  Split the n9ode using the feature that proivides the best split 
				according to the objective function, for instance, maximizing the information gain
		3. Repeat steps 1-2 k times.
		4. Aggregate the predicition by each tree to assign the class label by 
			majority vote. 
			
		Note: 	on step 2, we don't evaluate all features to determine the best split at each node,
				we only consider a random subset of those

		Replacement just means you put the item back into the collection after each selection
		
		Typically, the larger the number of trees, the better the performance of the random forest
		classifier at the expense of an increased hyperparameter cost.
		
		Don't need to prone since the ensemble model is quite robust to noise from the individual 
		decision trees
		
		Optimize random forest through (step 1) the number of features, d, 
		that are randomly chosen for each split (step 2.a)
		
		Via the ample size, n, of the bootstrap sample, we control 
		he bias-variance tradeoff of the random forest
		
		Decreasing bootstrap size increases diversity of the individual trees
		
		Usually, size of the bootstrap model is choosen to be equal to 
		the number of training exmaples in OG training set
		
		CODE
		------------------------------
		from sklearn.ensemble import RandomForestClassifier
		forest = RandomForestClassifier(criterion='gini'
										n_estimators=25,
										random_state = 1,
										n_jobs=2)
		
		forest.fit(X_train, y_train)
		plot_decision_regions(X_combined, y_combined,
								classifier=forest, test_idx=range(105,150))
		
		plt.xlabel('petal length [cm]')
		plt.ylabel('petal width [cm]')
		plt.legend(loc='upper left')
		plt.tight_layout()
		plt.show()
		
		Using the preceding code, we trained a random forest form 25 decison tres via the n_estimators
		parameter and used the Gini immpurity measure as a criterion to split the nodes.
		Although we are growing a very small random forest from a very small training dataset,
		we used the n_jobs param for demonstration purposes, which allows us to parrellelize 
		the model training using multiple cores of our computer (here, 2 cores)
		
		
	
	K-nearest neighbors - a lazy learning algorithm
	------------------------------------------------------------
	Called lazy learning becaause it doesn't learn a discriminative function from
	the training data but memorizes the trianing dataset instead.
	

	Summarized in these steps:
		1. Choose the number of k and a distance metric
		2. Find the k-nearest neighbors of the data record that we want to classify
		3. Assign the class label by majority vote
		
	Advantages of memory based approach: classifier immediately adapts as we collect new training data.
	
										***Downside is that computational complexity for classifying 
										new examples grows linearly with the number of examples in 
										the training data set
										
										Storage space could also be an issue
	
	k-d trees
	---------------------
	An Algorithm for Finding the BEst Matches in Logarithmic Expected Time, J.H Friedman, J.L. Bentley, and R.A Finkel,
	ACM Trainsactions on matehmatical software (TOMS), 3(3):209-226,1977
	
	
	Parametric vs. Nonparametric models
	--------------------------------------------------
	parametric : estimate parameters from the training dataset to learn a function that can classify 
				 new data points without requiring the original training dataset anymore
	
	nonparametric: can't be characterized by a fixed set of parameters, and the number of parameters
				   grows with the training data.
				   
				   		examples:tree classifier/ random forst and the kernel SVM
	
	
	Implement KNN model using Euclidean Distance metric
	-------------------------------------------------------------------------
	from sklearn.neightbors import KNeighborsClassifier
	knn = KNeighborsClassifer (n_neighbors = 5, p=2, metric = 'minkowski')
	knn.fit(X_train_std, y_train)
	plot_decision_regions(X_comnbined_std, y_combined, classifier=knn, test_idx=range(105,150)
	plt.xlabel('petal length [standardized]')
	plt.ylabel('petal width [standardized]')
	plt.legend(loc='upper left')
	plt.tight_layout()
	plt.show()
	
	
	Resolving ties: In the case of a tie, the scikit-learn implementation of the KNN algorithm
	will prefer the neighbors with a closer distnace to thbe data record to be classified. if the neighgbros havf similkar
	distances, the algorithm will choose the class lkabel that ocmes first in the training dataset.
	
	Euclidean distance : p=2
	Manhattan distance : p=1
	
	More metrics:
		https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html
	
	The curse of dimensionality : this is what makes knn prone to overfitting.
	
								  This phenomena is classified by the features space being 
								  increasingly sparse for an increasing number of dimension of a fixed-size training dataset
								  
								  Can use feature selection and dimensionality reduction techniques to help us to avoid this
				   		












