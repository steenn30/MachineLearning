	Other optimization algorithms
		1. newton-cg
		2. lbfgs (limited-memory- Broyden-Fletcher-Goldfarb-Shanno (BFGS)
		3. liblinear
		4. sag
		5. saga
		

	Logistic regression loss is convex, but algs most should converge 
	to the global loss minimum
	
	underfitting (high bias): when our model is not complex enough to capture the pattern 
							  in the training data well and therefore also suffers from 
							  low performance on unseen data
							  
	
	bias-variance tradeoff
	-----------------------------------
	
	- variance: measures the consistentcy (or variability) of the model prediction for 
				classifying a particular example if we retrain the model multiple times
			- We can say that the model is sensitive to the randomness in thet raining data
			
			
	-bias: meausres how far off the predicitons are form the correct values in general if we 
		  rebuild the model multiple times on different training datasets;
		  bias is the measure of systematic erro4r that is not due to randomness
	
	
	More info on these: https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf
	
	
	Regularization is a very useful method for handling collineraity 
	(high correlation among features), filtering out noise from data,
    and eventually preventing overfitting
    
    The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values
    
    L2 regularization(sometimes also called L2 shrinkage or weight decay
    	equation:
				(lambda/2) * ||w||^2 = (lambda/2) [summation(from: j=1, to: m).(w.subscript(j))^2]
				
		lambda is the regularization parameter, which shrinks the weights during model training
		increasing this parameter increases regularization strenght
		- the term c is the inverse
		
		
	
	
	
	Support Vector Machines
	------------------------------------
	- can be considered an extension of the perceptron
	- Perceptron is used to minimize misclassification errors, but SVM maximizes margin
	
	- margin :  distance between the seprating hyperplane (decision boundary) and the training 
				examples that are closest to this hyperplane, which are called support vectors
				
	- large margins typicallyt have a lower general;ization error
	- small margins are more prone to overfitting
	
	Read more about SVMS in:
				*	The NAture of Statistical Learning Theory, Springer Science+Business Media, Vladimir Vapnik, 2000
				
				*	A Tutorial on Support Vector Machines for 
					PAttern Recognition(Data Mining and Knowledge Discovery, 2(2): 121-167, 1998
					
					
	END
	
	Dealing with a nonlineraly sepratable case using slack variables
	---------------------------------------------------------------------------
	Slack variable led to the so-called *soft-margin classification*
	-   The motivation for intorducing the slack variable was that the linear constraints need to be relaxed for nonlinearly 
		separable data to allow the convergence of the optimization in the presence of misclassifications, 
		under appropriate cost penalization
		
	
	END
	
	Train SVM model to classify different flowers:
	----------------------------------
	from sklearn.svm import SVC
	svm = SVC(kernel='linear', C=1.0, random_state=1)
	svm.fit(X_train_std, y_train)
	plot_decision_regions(X_combined_std,
							y_combined,
							classifier=svm
							test_idx=range(105,150)
	plt.xlabel('petal length [standardized]')
	plt.ylabel('petal width [standardized]'
	plt.legend(loc='upper left')
	plt.tight_layout()
	plt.show()
	
	NOTES
	---------------------------
	Linear logistic regression and linear SVMs often yield very similar results. 
	Logistic regression tries to maximize the conditional likelihoods of the training data, 
	which makes it more prone to outliers than SVMs, which mostly care about the points that are closest to 
	the decision boundary(support vectors). 
	
	On the other hand, logistic regression has the advantage that it is a simpler model and can be 
	implemented more easily. Furthermore, logistic regression models can be easily updated, 
	which is attractive when working with streaming data.














